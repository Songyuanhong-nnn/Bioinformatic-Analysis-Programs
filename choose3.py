import os
import shutil
import zipfile
import hashlib
import argparse
from typing import List, Dict, Optional
from pathlib import Path

# ===================== é€šç”¨é…ç½®ï¼ˆå¯æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ï¼‰=====================
SUPPORTED_ARCHIVES = (".zip", ".gz", ".tar.gz", ".tgz")  # æ”¯æŒçš„å‹ç¼©æ ¼å¼ï¼ˆæ‰©å±•é€šç”¨ï¼‰
SUPPORTED_EXTENSIONS = (".fna", ".fasta", ".fa")  # æ”¯æŒçš„åºåˆ—æ–‡ä»¶æ ¼å¼ï¼ˆæ‰©å±•é€šç”¨ï¼‰
TEMP_DIR_NAME = "temp_process"  # ä¸´æ—¶ç›®å½•åç§°
SUMMARY_FILE_NAME = "fna_extract_summary.txt"  # æ€»ç»“æŠ¥å‘Šåç§°

# ===================== é€šç”¨å·¥å…·å‡½æ•° =====================
def calculate_md5(file_path: str) -> str:
    """é€šç”¨MD5è®¡ç®—å‡½æ•°ï¼Œå…¼å®¹å¤§æ–‡ä»¶"""
    md5_hash = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                md5_hash.update(chunk)
        return md5_hash.hexdigest()
    except Exception as e:
        print(f"âš ï¸  è®¡ç®—æ–‡ä»¶MD5å¤±è´¥ï¼š{file_path} -> {str(e)}")
        return ""

def validate_dir_structure(root_dir: str) -> Dict[str, str]:
    """
    è‡ªåŠ¨æ ¡éªŒç›®å½•ç»“æ„ï¼Œè¿”å›æ ¸å¿ƒç›®å½•è·¯å¾„
    æ ¸å¿ƒçº¦å®šï¼šroot_dir ä¸‹å¿…é¡»åŒ…å« rawsource æ–‡ä»¶å¤¹ï¼Œè„šæœ¬æ‰€åœ¨ç›®å½•ä¸º tools
    """
    # è‡ªåŠ¨è¯†åˆ«æ ¸å¿ƒç›®å½•ï¼ˆå…¼å®¹è„šæœ¬æ”¾åœ¨ tools æˆ–å…¶ä»–ä½ç½®ï¼Œåªè¦ root_dir æ­£ç¡®ï¼‰
    dirs = {
        "rawsource": os.path.join(root_dir, "rawsource"),
        "clean_genome": os.path.join(root_dir, "clean_genome"),
        "temp": os.path.join(root_dir, TEMP_DIR_NAME)
    }

    # æ ¡éªŒåŸå§‹æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dirs["rawsource"]):
        raise FileNotFoundError(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼š{dirs['rawsource']}\nè¯·ç¡®ä¿ root_dir ä¸‹åŒ…å« 'rawsource' æ–‡ä»¶å¤¹")

    # è‡ªåŠ¨åˆ›å»ºç›®æ ‡ç›®å½•å’Œä¸´æ—¶ç›®å½•
    for dir_path in dirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    print("âœ… ç›®å½•ç»“æ„æ ¡éªŒé€šè¿‡ï¼š")
    for name, path in dirs.items():
        print(f"  - {name}: {path}")
    return dirs

def get_existing_file_md5(target_dir: str) -> set:
    """é€šç”¨å‡½æ•°ï¼šè·å–ç›®æ ‡ç›®å½•ä¸­æ‰€æœ‰æ”¯æŒæ ¼å¼æ–‡ä»¶çš„MD5é›†åˆ"""
    existing_md5 = set()
    for root, _, files in os.walk(target_dir):
        for file in files:
            if file.lower().endswith(SUPPORTED_EXTENSIONS):
                file_path = os.path.join(root, file)
                md5_val = calculate_md5(file_path)
                if md5_val:
                    existing_md5.add(md5_val)
    print(f"âœ… æ£€æµ‹åˆ°ç›®æ ‡ç›®å½•ä¸­å·²æœ‰ {len(existing_md5)} ä¸ªå”¯ä¸€åºåˆ—æ–‡ä»¶ï¼Œå°†è·³è¿‡é‡å¤æ–‡ä»¶")
    return existing_md5

def extract_archive(archive_path: str, extract_dir: str) -> None:
    """é€šç”¨è§£å‹å‡½æ•°ï¼šæ”¯æŒ zip/gz/tar.gz/tgz æ ¼å¼"""
    archive_ext = os.path.splitext(archive_path.lower())
    try:
        # å¤„ç† zip æ ¼å¼
        if archive_ext[-1] == ".zip":
            with zipfile.ZipFile(archive_path, "r") as zf:
                zf.extractall(extract_dir)
        # å¤„ç† gz/tar.gz/tgz æ ¼å¼
        elif archive_ext[-1] == ".gz" or (len(archive_ext) > 1 and archive_ext[-2] == ".tar"):
            import tarfile  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªä½¿ç”¨æ—¶åŠ è½½
            mode = "r:gz" if archive_path.lower().endswith((".tar.gz", ".tgz")) else "r:gz"
            with tarfile.open(archive_path, mode) as tf:
                tf.extractall(extract_dir)
        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„å‹ç¼©æ ¼å¼ï¼š{archive_path}")
            return
        print(f"âœ… è§£å‹æˆåŠŸï¼š{os.path.basename(archive_path)}")
    except Exception as e:
        print(f"âŒ è§£å‹å¤±è´¥ {os.path.basename(archive_path)}ï¼š{str(e)}")

def find_all_archives(source_dir: str) -> List[str]:
    """é€šç”¨å‡½æ•°ï¼šé€’å½’æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„å‹ç¼©åŒ…ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å¤¹ï¼‰"""
    archives = []
    for root, _, files in os.walk(source_dir):
        for file in files:
            if file.lower().endswith(SUPPORTED_ARCHIVES):
                archives.append(os.path.join(root, file))
    return archives

def copy_existing_folders(source_dir: str, target_dir: str) -> None:
    """é€šç”¨å‡½æ•°ï¼šå¤åˆ¶å·²è§£å‹çš„æ–‡ä»¶å¤¹ï¼ˆè·³è¿‡å‹ç¼©åŒ…ï¼‰"""
    for item in os.listdir(source_dir):
        item_path = os.path.join(source_dir, item)
        # è·³è¿‡å‹ç¼©åŒ…ï¼Œåªå¤„ç†æ–‡ä»¶å¤¹
        if os.path.isdir(item_path) and not any(item.lower().endswith(ext) for ext in SUPPORTED_ARCHIVES):
            dest_path = os.path.join(target_dir, item)
            # é¿å…é‡å¤å¤åˆ¶ï¼Œå…ˆåˆ é™¤å·²å­˜åœ¨çš„åŒåæ–‡ä»¶å¤¹
            if os.path.exists(dest_path):
                shutil.rmtree(dest_path)
            try:
                shutil.copytree(item_path, dest_path, ignore=shutil.ignore_patterns(*[f"*{ext}" for ext in SUPPORTED_ARCHIVES]))
                print(f"âœ… å¤åˆ¶æ–‡ä»¶å¤¹æˆåŠŸï¼š{item}")
            except Exception as e:
                print(f"âŒ å¤åˆ¶æ–‡ä»¶å¤¹å¤±è´¥ {item}ï¼š{str(e)}")

def extract_target_files(extract_dir: str, target_dir: str, existing_md5: set) -> List[str]:
    """é€šç”¨å‡½æ•°ï¼šæå–æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡ä»¶ï¼ˆå»é‡ã€åŒåå¤„ç†ï¼‰"""
    new_files = []
    for root, _, files in os.walk(extract_dir):
        for file in files:
            if file.lower().endswith(SUPPORTED_EXTENSIONS):
                source_path = os.path.join(root, file)
                current_md5 = calculate_md5(source_path)
                
                # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
                if not current_md5 or current_md5 in existing_md5:
                    print(f"âš ï¸  è·³è¿‡é‡å¤æ–‡ä»¶ï¼š{file}")
                    continue
                
                # å¤„ç†åŒåæ–‡ä»¶ï¼ˆæ·»åŠ åºå·ï¼‰
                target_path = os.path.join(target_dir, file)
                counter = 1
                while os.path.exists(target_path):
                    name, ext = os.path.splitext(file)
                    target_path = os.path.join(target_dir, f"{name}_{counter}{ext}")
                    counter += 1
                
                # å¤åˆ¶æ–‡ä»¶ï¼ˆä¿ç•™å…ƒæ•°æ®ï¼‰
                shutil.copy2(source_path, target_path)
                new_files.append(target_path)
                existing_md5.add(current_md5)
                print(f"âœ… æå–æ–°æ–‡ä»¶ï¼š{os.path.basename(source_path)} {'-> ' + os.path.basename(target_path) if counter > 1 else ''}")
    
    if new_files:
        print(f"\nâœ… æœ¬æ¬¡å…±æå– {len(new_files)} ä¸ªæ–°æ–‡ä»¶åˆ° {target_dir}")
    else:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°æ–°çš„åºåˆ—æ–‡ä»¶")
    return new_files

def remove_duplicate_files(target_dir: str) -> tuple[int, int, int]:
    """é€šç”¨å»é‡å‡½æ•°ï¼šåˆ é™¤ç›®æ ‡ç›®å½•ä¸­é‡å¤çš„æ”¯æŒæ ¼å¼æ–‡ä»¶"""
    md5_record = {}
    duplicate_count = 0
    total_count = 0

    for root, _, files in os.walk(target_dir):
        for file in files:
            if file.lower().endswith(SUPPORTED_EXTENSIONS):
                total_count += 1
                file_path = os.path.join(root, file)
                md5_val = calculate_md5(file_path)

                if md5_val in md5_record:
                    try:
                        os.remove(file_path)
                        duplicate_count += 1
                        print(f"âŒ åˆ é™¤é‡å¤æ–‡ä»¶ï¼š{file}ï¼ˆä¸ {os.path.basename(md5_record[md5_val])} é‡å¤ï¼‰")
                    except Exception as e:
                        print(f"âš ï¸  åˆ é™¤é‡å¤æ–‡ä»¶å¤±è´¥ {file}ï¼š{str(e)}")
                elif md5_val:
                    md5_record[md5_val] = file_path
    
    unique_count = total_count - duplicate_count
    print(f"\nâœ… å»é‡å®Œæˆï¼šå…±å¤„ç† {total_count} ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤ {duplicate_count} ä¸ªé‡å¤æ–‡ä»¶ï¼Œä¿ç•™ {unique_count} ä¸ªå”¯ä¸€æ–‡ä»¶")
    return total_count, duplicate_count, unique_count

def generate_summary_report(target_dir: str, total_extracted: int, total_duplicated: int, total_unique: int) -> None:
    """é€šç”¨æ€»ç»“æŠ¥å‘Šå‡½æ•°ï¼šç”Ÿæˆè¯¦ç»†çš„æå–æŠ¥å‘Š"""
    summary_path = os.path.join(target_dir, SUMMARY_FILE_NAME)
    total_size = 0
    unique_files = []

    # ç»Ÿè®¡å”¯ä¸€æ–‡ä»¶ä¿¡æ¯
    for root, _, files in os.walk(target_dir):
        for file in files:
            if file.lower().endswith(SUPPORTED_EXTENSIONS):
                file_path = os.path.join(root, file)
                unique_files.append((file, os.path.getsize(file_path), root))
                total_size += os.path.getsize(file_path)
    
    # å•ä½è½¬æ¢ï¼ˆå­—èŠ‚ -> MBï¼‰
    total_size_mb = total_size / (1024 * 1024)

    # å†™å…¥æŠ¥å‘Š
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write("åºåˆ—æ–‡ä»¶æå–ä¸å»é‡æ€»ç»“æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n")
        f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{os.popen('date').read().strip() if os.name != 'nt' else os.popen('date /t && time /t').read().strip()}\n")
        f.write(f"1. æå–ç»Ÿè®¡ï¼šæœ¬æ¬¡å…±æå– {total_extracted} ä¸ªæ–°åºåˆ—æ–‡ä»¶\n")
        f.write(f"2. å»é‡ç»Ÿè®¡ï¼šåˆ é™¤ {total_duplicated} ä¸ªé‡å¤æ–‡ä»¶ï¼Œå½“å‰ä¿ç•™ {total_unique} ä¸ªå”¯ä¸€æ–‡ä»¶\n")
        f.write(f"3. å¤§å°ç»Ÿè®¡ï¼šå”¯ä¸€æ–‡ä»¶æ€»å¤§å° {total_size_mb:.2f} MB\n")
        f.write(f"4. å­˜å‚¨è·¯å¾„ï¼š{os.path.abspath(target_dir)}\n")
        f.write(f"5. æ”¯æŒæ ¼å¼ï¼š{', '.join(SUPPORTED_EXTENSIONS)}\n")
        f.write(f"6. å”¯ä¸€æ–‡ä»¶åˆ—è¡¨ï¼ˆå…± {len(unique_files)} ä¸ªï¼‰ï¼š\n")
        for i, (file, size, root) in enumerate(unique_files, 1):
            size_mb = size / (1024 * 1024)
            f.write(f"   {i:3d}. æ–‡ä»¶åï¼š{file} | å¤§å°ï¼š{size_mb:.2f} MB | è·¯å¾„ï¼š{os.path.relpath(root, target_dir)}\n")
        f.write("=" * 60 + "\n")
    
    print(f"\nâœ… æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆï¼š{summary_path}")

def clean_temp_dir(temp_dir: str) -> None:
    """é€šç”¨æ¸…ç†å‡½æ•°ï¼šå®‰å…¨åˆ é™¤ä¸´æ—¶ç›®å½•"""
    if os.path.exists(temp_dir):
        try:
            shutil.rmtree(temp_dir)
            print(f"\nâœ… å·²æ¸…ç†ä¸´æ—¶ç›®å½•ï¼š{temp_dir}")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥ï¼š{str(e)}")

# ===================== ä¸»å‡½æ•°ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰=====================
def main(root_dir: Optional[str] = None):
    """
    ä¸»å‡½æ•°ï¼šæ”¯æŒä¸¤ç§è¿è¡Œæ–¹å¼
    1. ç›´æ¥è¿è¡Œï¼šè‡ªåŠ¨è¯†åˆ«è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ä¸º root_dir
    2. å‘½ä»¤è¡ŒæŒ‡å®šï¼šé€šè¿‡ --root å‚æ•°æŒ‡å®š root_dir
    """
    # è‡ªåŠ¨è¯†åˆ« root_dirï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if not root_dir:
        # è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆtoolsæ–‡ä»¶å¤¹ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # root_dir ä¸º tools çš„ä¸Šçº§ç›®å½•ï¼ˆåŒ…å« rawsource å’Œ clean_genomeï¼‰
        root_dir = os.path.dirname(script_dir)
    
    print("=" * 70)
    print("ğŸ¯ é€šç”¨åºåˆ—æ–‡ä»¶å¢é‡æå–å·¥å…·ï¼ˆæ”¯æŒ NCBI æ•°æ®ï¼‰")
    print(f"ğŸ“ æ ¹ç›®å½•ï¼š{root_dir}")
    print("=" * 70 + "\n")

    try:
        # æ­¥éª¤1ï¼šæ ¡éªŒç›®å½•ç»“æ„
        print("ã€æ­¥éª¤1/6ã€‘æ ¡éªŒç›®å½•ç»“æ„...")
        dirs = validate_dir_structure(root_dir)
        rawsource_dir = dirs["rawsource"]
        clean_dir = dirs["clean_genome"]
        temp_dir = dirs["temp"]

        # æ­¥éª¤2ï¼šè·å–å·²å­˜åœ¨æ–‡ä»¶çš„MD5ï¼ˆå¢é‡æå–æ ¸å¿ƒï¼‰
        print("\nã€æ­¥éª¤2/6ã€‘æ£€æµ‹å·²å­˜åœ¨çš„åºåˆ—æ–‡ä»¶...")
        existing_md5 = get_existing_file_md5(clean_dir)

        # æ­¥éª¤3ï¼šè§£å‹æ‰€æœ‰å‹ç¼©åŒ…
        print("\nã€æ­¥éª¤3/6ã€‘è§£å‹æ‰€æœ‰æ”¯æŒçš„å‹ç¼©åŒ…...")
        archives = find_all_archives(rawsource_dir)
        if archives:
            print(f"æ‰¾åˆ° {len(archives)} ä¸ªå‹ç¼©åŒ…ï¼Œå¼€å§‹è§£å‹...")
            for archive in archives:
                extract_archive(archive, temp_dir)
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„å‹ç¼©åŒ…ï¼Œè·³è¿‡è§£å‹æ­¥éª¤")

        # æ­¥éª¤4ï¼šå¤åˆ¶å·²è§£å‹çš„æ–‡ä»¶å¤¹
        print("\nã€æ­¥éª¤4/6ã€‘å¤åˆ¶å·²è§£å‹çš„æ–‡ä»¶å¤¹...")
        copy_existing_folders(rawsource_dir, temp_dir)

        # æ­¥éª¤5ï¼šæå–æ–°æ–‡ä»¶ï¼ˆå»é‡ã€åŒåå¤„ç†ï¼‰
        print("\nã€æ­¥éª¤5/6ã€‘æå–æ–°çš„åºåˆ—æ–‡ä»¶...")
        new_files = extract_target_files(temp_dir, clean_dir, existing_md5)
        total_extracted = len(new_files)

        # æ­¥éª¤6ï¼šå»é‡ + ç”ŸæˆæŠ¥å‘Š
        print("\nã€æ­¥éª¤6/6ã€‘å»é‡å¹¶ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        total_count, total_duplicated, total_unique = remove_duplicate_files(clean_dir)
        generate_summary_report(clean_dir, total_extracted, total_duplicated, total_unique)

        # æ¸…ç†ä¸´æ—¶ç›®å½•
        clean_temp_dir(temp_dir)

        print(f"\nğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼æœ€ç»ˆç»“æœä¿å­˜åœ¨ï¼š{clean_dir}")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå¤±è´¥ï¼š{str(e)}")
        # å¼‚å¸¸æ—¶ä¹Ÿå°è¯•æ¸…ç†ä¸´æ—¶ç›®å½•
        if 'temp_dir' in locals() and os.path.exists(temp_dir):
            clean_temp_dir(temp_dir)
        exit(1)

# ===================== å‘½ä»¤è¡Œæ”¯æŒï¼ˆå¢å¼ºé€šç”¨æ€§ï¼‰=====================
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="é€šç”¨åºåˆ—æ–‡ä»¶å¢é‡æå–å·¥å…·ï¼ˆæ”¯æŒ FNA/FASTA/FA æ ¼å¼ï¼Œè‡ªåŠ¨è§£å‹å‹ç¼©åŒ…ï¼‰")
    parser.add_argument("--root", type=str, help="æŒ‡å®šæ ¹ç›®å½•ï¼ˆåŒ…å« rawsource å’Œ clean_genome çš„ç›®å½•ï¼Œå¯é€‰ï¼‰")
    args = parser.parse_args()

    # è¿è¡Œä¸»å‡½æ•°
    main(root_dir=args.root)